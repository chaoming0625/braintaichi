{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BrainTaichi Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial provides a comprehensive guide on how to develop custom operators using `BrainTaichi`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Registration Interface\n",
    "\n",
    "Brain dynamics is sparse and event-driven, however, proprietary operators for brain dynamics are not well abstracted and summarized. As a result, we are often faced with the need to customize operators. In this tutorial, we will explore how to customize brain dynamics operators using `BrainTaichi`.\n",
    "\n",
    "Start by importing the relevant Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import braintaichi as bti\n",
    "import brainstate as bst\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import pytest\n",
    "import platform\n",
    "\n",
    "import taichi as ti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Structure of Custom Operators\n",
    "`Taichi` uses Python functions and decorators to define custom operators. Here is a basic structure of a custom operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ti.kernel\n",
    "def my_kernel(arg1: ti.types.ndarray(), arg2: ti.types.ndarray()):\n",
    "    # Internal logic of the operator\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@ti.kernel` decorator tells Taichi that this is a function that requires special compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Helper Functions\n",
    "When defining complex custom operators, you can use the @ti.func decorator to define helper functions. These functions can be called inside the kernel function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ti.func\n",
    "def helper_func(x: ti.f32) -> ti.f32:\n",
    "    # Auxiliary computation\n",
    "    return x * 2\n",
    "\n",
    "@ti.kernel\n",
    "def my_kernel(arg: ti.types.ndarray()):\n",
    "    for i in ti.ndrange(arg.shape[0]):\n",
    "        arg[i] *= helper_func(arg[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Custom Event Processing Operator\n",
    "The following example demonstrates how to customize an event processing operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ti.func\n",
    "def get_weight(weight: ti.types.ndarray()) -> ti.f32:\n",
    "    return weight[None]\n",
    "\n",
    "@ti.func\n",
    "def update_output(out: ti.types.ndarray(), index: ti.i32, weight_val: ti.f32):\n",
    "    out[index] += weight_val\n",
    "\n",
    "@ti.kernel\n",
    "def event_ell_cpu(indices: ti.types.ndarray(),\n",
    "                  vector: ti.types.ndarray(),\n",
    "                  weight: ti.types.ndarray(),\n",
    "                  out: ti.types.ndarray()):\n",
    "    weight_val = get_weight(weight)\n",
    "    num_rows, num_cols = indices.shape\n",
    "    ti.loop_config(serialize=True)\n",
    "    for i in range(num_rows):\n",
    "        if vector[i]:\n",
    "            for j in range(num_cols):\n",
    "                update_output(out, indices[i, j], weight_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the declaration of parameters, the last few parameters need to be output parameters so that `Taichi` can compile correctly. This operator `event_ell_cpu` receives indices, vectors, weights, and output arrays, and updates the output arrays according to the provided logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering and Using Custom Operators\n",
    "After defining a custom operator, it can be registered into a specific framework and used where needed. \n",
    "`BrainTaichi` provides a simple and flexible interface for registering custom operators -- `XLACustomOp`. When registering, you can specify `cpu_kernel` and `gpu_kernel`, so the operator can run on different devices. Specify the outs parameter when calling, using `jax.ShapeDtypeStruct` to define the shape and data type of the output.\n",
    "\n",
    "Note: Maintain the order of the operatorâ€™s declared parameters consistent with the order when calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taichi operator registration\n",
    "prim = bti.XLACustomOp(cpu_kernel=event_ell_cpu, gpu_kernel=event_ell_gpu)\n",
    "\n",
    "# Using the operator\n",
    "def test_taichi_op():\n",
    "    # Create input data\n",
    "    # ...\n",
    "\n",
    "    # Call the custom operator\n",
    "    out = prim(indices, vector, weight, outs=[jax.ShapeDtypeStruct((s,), dtype=jnp.float32)])\n",
    "\n",
    "    # Output the result\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Taichi Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taichi is a domain-specific language (DSL) designed to simplify the development of high-performance visual computing and physics simulation algorithms, particularly for computer graphics researchers. Here are some of the basic concepts of Taichi based on the provided introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedded in Python\n",
    "Taichi is embedded within Python, allowing developers to leverage the simplicity and flexibility of Python while benefiting from the performance of native GPU or CPU instructions. This means that if you are familiar with Python, you can quickly start using Taichi without learning a completely new language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just-in-Time (JIT) Compilation\n",
    "Taichi uses modern JIT compilation frameworks like LLVM and SPIR-V to translate Python code into native GPU or CPU instructions. This approach ensures that the code runs efficiently both during development and at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imperative Programming Paradigm\n",
    "Unlike many other DSLs that focus on specific computing patterns, Taichi adopts an imperative programming paradigm. This provides greater flexibility and allows developers to write complex computations in a single kernel, which Taichi refers to as a \"mega-kernel.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizations\n",
    "Taichi employs various compiler optimizations such as common subexpression elimination, dead code elimination, and control flow graph analysis. These optimizations are backend-neutral, thanks to Taichi's own intermediate representation (IR) layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community and Backend Support\n",
    "Taichi has a strong and dedicated community that has contributed to the development of various backends, including Vulkan, OpenGL, and DirectX. This wide range of backend support enhances Taichi's portability and usability across different platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Optimization in Taichi\n",
    "\n",
    "`Taichi` kernels automatically parallelize for-loops in the outermost scope. Our compiler sets the settings automatically to best explore the target architecture. Nonetheless, for Ninjas seeking the final few percent of speed, we provide several APIs to allow developers to fine-tune their programs. Specifying a proper block_dim is key.\n",
    "\n",
    "You can use `ti.loop_config` to set the loop directives for the next for loop. Available directives are:\n",
    "\n",
    "- **parallelize**: Sets the number of threads to use on CPU\n",
    "- **block_dim**: Sets the number of threads in a block on GPU\n",
    "- **serialize**: If you set **serialize** to True, the for loop will run serially, and you can write break statements inside it (Only applies on range/ndrange fors). Equals to setting **parallelize** to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ti.kernel\n",
    "def break_in_serial_for() -> ti.i32:\n",
    "    a = 0\n",
    "    ti.loop_config(serialize=True)\n",
    "    for i in range(100):  # This loop runs serially\n",
    "        a += i\n",
    "        if i == 10:\n",
    "            break\n",
    "    return a\n",
    "\n",
    "break_in_serial_for()  # returns 55\n",
    "n = 128\n",
    "val = ti.field(ti.i32, shape=n)\n",
    "@ti.kernel\n",
    "def fill():\n",
    "    ti.loop_config(parallelize=8, block_dim=16)\n",
    "    # If the kernel is run on the CPU backend, 8 threads will be used to run it\n",
    "    # If the kernel is run on the CUDA backend, each block will have 16 threads.\n",
    "    for i in range(n):\n",
    "        val[i] = i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainpy-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
